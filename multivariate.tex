\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\title[Multivariate Probabilty and Statistics]{Multivariate Probabilty and Statistics}
\author{Jussi Martin}
%\institute{Where You're From}
\date{October 24, 2016}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

% \section{Introduction}
%
% \begin{frame}{Introduction}
%
% \begin{itemize}
%   \item Your introduction goes here!
%   \item Use \texttt{itemize} to organize your main points.
% \end{itemize}
%
% \vskip 1cm
%
% \begin{block}{Examples}
% Some examples of commonly used commands and features are included, to help you get started.
% \end{block}
%
% \end{frame}

\section{Introduction}

\begin{frame}{Introduction}
  This presentation is a breef summary of some of the topics introduced in
  chapter 4 of the book \emph{Natural Image Statistics} (reference at
  the last slide).

  Since there won't be enough time to cover the whole chapter 4, I decide to go
  trough the things that are needed for understanding the \emph{Baye's rule},
  which I think is one of the core content in this chapter.

  I will also introduce \emph{expectation} and \emph{variance} in these slides,
  since they are needed in the exercises.
\end{frame}

\section{Random Variables}

\begin{frame}{Random Variables}
 Discrete random variable is a variable which can have any value from some given
 range with some given probabilty. Example: outcome of throwing a dice. The outcome
 can have any value from 1 to 6, each with the probability $\frac{1}{6}$.

 Continuos random variable is a variable which can have any value from some given
 continuous range of values, with some given probabilities for the value been in
 any given interval. Example WRITE THIS!
\end{frame}

\section{Random Vectors}

\begin{frame}{Random Vectors}
  Random vector is a $n$-tuple of random variables. Each of these can be either
  similar or different types of variables. Formally:
  \[ \mathbf{z} =  \begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{pmatrix} \]
  where $\mathbf{z}$ is the random vector and the random variables $z_i$ are
  its components.
\end{frame}

\section{Probability Density Function (pdf)}

\begin{frame}{Probability Density Function (pdf)}
  Let $P(\text{$z$ is in [a, b]})$ be the probability that the value of a
  random variable $z$ is in the interval $[a, b]$, then the probabilty density
  function $p_z(a)$ can be defined approximately as follows:
  \[ p_z(a) \approx \frac{P(\textnormal{$z$ is in $[a, a + \nu]$})}{\nu}\]
  for very small values of $\nu$.

  Similarily, if $\mathbf{z}$ is a random vector:
  \[ p_{\mathbf{z}}(\mathbf{a}) \approx
  \frac{P(\textnormal{$z_i$ is in $[a_i, a_i + \nu]$ for all $i$})}{\nu^n}\]
  for very small values of $\nu$. Rigorous definitions are obtained by taking
  the one-sided limit $\nu \to 0^+$.
\end{frame}

\section{Joint and Marginal pdfs}

\begin{frame}{Joint and Marginal pdfs}
  The pdf $p(z_1, z_2, \ldots, z_n)$ of a random vector $\mathbf{z}$ is also
  called joint pdf, since it depends on all the components $z_i$.

  The marginal pdfs $p_{z_i}(z_i)$ are obtained by integrating over the other
  variables (that is $z_j$, with $j \ne i$). Example in two dimensions:
  \[ p_{z_1}(z_1) =  \int p(z_1, z_2)dz_2\]
  or more rigorosly:
  \[ p_{z_1}(\nu_1) =  \int p(\nu_1, \nu_2)d\nu_2\]
\end{frame}

\section{Conditional Probabilities}

\begin{frame}{Conditional Probabilities}
  (continuous case, discrete case)
  \[
  p(z_2|z_1 = a) =
  \frac{ p_{\mathbf{z}}(a, z_2)}{\int p_{\mathbf{z}}(a, z_2)dz_2}
  \]
  or more concisely:
  \[
  p(z_2|z_1 = a) =
  \frac{ p_{\mathbf{z}}(a, z_2)}{p_{z_1}(a)} = \text{e.t.c.}
  \]
  for discrete case the integral is replaced by sum, that is
  \[ P_{z_1}(z_1) = \sum_{z_2}P_{\mathbf{z}}(z_1, z_2)\]
\end{frame}

\section{Independence}

\begin{frame}{Independence}
  definition
  \[
  p(z_2|z_1) = p(z_2) \ \text{for every $z_1$ and $z_2$}
  \]
  which implies that
  \[
  \frac{p(z_1, z_2)}{p(z_1)} = p(z_2) \ \text{or otherway around that} \ p(z_1, z_2) = p(z_1)p(z_2)
  \]
  for all $z_1$ and $z_2$.
\end{frame}

% \section{Covariance Matrix}
%
% \begin{frame}{Covariance Matrix}
%   \[
%   \mathbf{C}(\mathbf{z}) =
%   \begin{pmatrix}
%     cov(z_1, z_1) \ cov(z_1, z_2) \ldots cov(z_1, z_n) \\
%     cov(z_2, z_1) \ cov(z_2, z_2) \ldots cov(z_2, z_n) \\
%     \vdots \qquad \qquad \ddots \quad \vdots \\
%     cov(z_n, z_1) \ cov(z_n, z_2) \ldots cov(z_n, z_n)
%   \end{pmatrix}
%   \]
%   or more concisely:
%   \[ \mathbf{C}(\mathbf{z}) =
%   E\{\mathbf{zz}^T\} - E\{\mathbf{z}\} E\{\mathbf{z}\}^T\]
% \end{frame}
%
% \section{Independence and covariance}
%
% \begin{frame}{Independence and covariance}
%   For independent variables $z_1$ and $z_2$ the relation
%   \[
%   E\{g_1(z_1)g_2(z_2)\} = E\{g_1(z_1)\}E\{g_2(z_2)\}
%   \]
%   holds for any functions $g_1$ and $g_2$
%   (rigorously speaking: for any measurable functions), which implies that
%   indepandent variables are uncorreleted (take $g_1(z) = g_2(z) = z$).
% \end{frame}

\section{Baye's Rule}

\begin{frame}{Baye's Rule}
  Baye's rule
  \[
  p(\mathbf{s}|\mathbf{z}) =
  \frac{p(\mathbf{z}|\mathbf{s})p_{\mathbf{s}}(\mathbf{s})}{\int p(\mathbf{z}|\mathbf{s})p_{\mathbf{s}}(\mathbf{s})ds}
  \]
  Or more rigorously WRITE HERE?
\end{frame}

% \section{Non informative priors}
%
% \begin{frame}{Non informative priors}
%   definition
%   \[
%   p(\mathbf{s}|\mathbf{z}) =
%   \frac{p(\mathbf{z}|\mathbf{s})c}{\int p(\mathbf{z}|\mathbf{s})cds} =
%   \frac{p(\mathbf{z}|\mathbf{s})}{\int p(\mathbf{z}|\mathbf{s})ds}
%   \]
% \end{frame}
%
% \section{Bayesian inference as a incremental learning process}
%
% \begin{frame}{Bayesian inference as a incremental learning process}
%   Explanation
% \end{frame}
%
% \section{Parameter estimation and likelihood}
%
% \begin{frame}{Parameter estimation and likelihood}
%   (statistical model, estimation, likelihood \& log-likelihood)
% \end{frame}
%
% \section{Maximum likelihood and maximum posteriori}
%
% \begin{frame}{Maximum likelihood and maximum posteriori}
%   (definition, definition)
% \end{frame}
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Template examples}
%
% \begin{frame}{Template examples}
%   Examples from the template on the next slides:
% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%% Some of the original default code below: %%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \section{Some \LaTeX{} Examples}
%
% \subsection{Tables and Figures}
%
% \begin{frame}{Tables and Figures}
%
% \begin{itemize}
% \item Use \texttt{tabular} for basic tables --- see Table~\ref{tab:widgets}, for example.
% \item You can upload a figure (JPEG, PNG or PDF) using the files menu.
% \item To include it in your document, use the \texttt{includegraphics} command (see the comment below in the source code).
% \end{itemize}
%
% % Commands to include a figure:
% %\begin{figure}
% %\includegraphics[width=\textwidth]{your-figure's-file-name}
% %\caption{\label{fig:your-figure}Caption goes here.}
% %\end{figure}
%
% \begin{table}
% \centering
% \begin{tabular}{l|r}
% Item & Quantity \\\hline
% Widgets & 42 \\
% Gadgets & 13
% \end{tabular}
% \caption{\label{tab:widgets}An example table.}
% \end{table}
%
% \end{frame}
%
% \subsection{Mathematics}
%
% \begin{frame}{Readable Mathematics}
%
% Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
% $$S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%       = \frac{1}{n}\sum_{i}^{n} X_i$$
% denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.
%
% \end{frame}

\section{Appendix I, Expectation}

\begin{frame}{Appendix I, Expectation}
  Expectation $E\{z\}$ of a random variable $z$ is a weighted average of the
  outcomes which the variable can obtain, with the weights been the individual
  propabilities (or values of the pdf in the continuous case).
  \[
  \text{Discrete case:} \qquad
  E\{z\} = \sum_{z} P_{z}(z)z
  \]
  \[
  \text{Continuous case:} \qquad
  E\{z\} = \int p_{z}(z)zdz
  \]
  The expectation can be also defined for random vectors component wise:
  \[
  E\{\mathbf{z}\} =
  \begin{pmatrix} E\{z_1\} \\ E\{z_2\} \\ \vdots \\ E\{z_n\} \end{pmatrix}
  \]
  where $E\{z_i\}$ is defined as above, with $z$ repleced by $z_i$ for all $i$.
  % (write the components?)\\
  % it is linear:
  % \[
  % E\{a\mathbf{z} + b\mathbf{z}\}  = aE\{\mathbf{z}\} + bE\{\mathbf{z}\}.
  % \]
  % Similarily, for any matrix $\mathbf{M}$
  % \[
  % E\{\mathbf{Mz}\} = \mathbf{M}E\{\mathbf{z}\}.
  % \]
\end{frame}

\section{Appendix II, Variance}

\begin{frame}{Appendix II, Variance}
  Variance in some sense tells how close the probability mass is concentrated
  near the ecpectation value. It is defined as follows:
  \[
  var(z)= E\{z^2\}-(E\{z\})^2.
  \]
  Alternatively it can defined as
  \[
   var(z) = E\{(z-E\{z\})^2\}.
  \]
  WRITE THIS IN A BETTER WAY!
  % covariance and correlation
  % \[
  % cov(z_1, z_2) = E\{z_1 z_2\} - E\{z_1\}E\{z_2\}
  % \]
  % \[
  % corr(z_1,z_2) = \frac{cov(z_1, z_2)}{\sqrt{var(z_1)var(z_2)}}
  % \]

\end{frame}

\section{References}

\begin{frame}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{09}
\bibitem[1]{p1} Aapo Hyvärinen, Jarmo Hurri, and Patrik O. Hoyer
\newblock Natural Image Statistics: A Probabilistic Approach to Early Computational Vision
\newblock \emph{Springer-Verlag}, 2009
\end{thebibliography}
}
\end{frame}

\end{document}
