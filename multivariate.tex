\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\title[Multivariate probabilty and statistics]{Multivariate probabilty and statistics}
\author{Jussi Martin}
%\institute{Where You're From}
\date{Date of Presentation}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

% \section{Introduction}
%
% \begin{frame}{Introduction}
%
% \begin{itemize}
%   \item Your introduction goes here!
%   \item Use \texttt{itemize} to organize your main points.
% \end{itemize}
%
% \vskip 1cm
%
% \begin{block}{Examples}
% Some examples of commonly used commands and features are included, to help you get started.
% \end{block}
%
% \end{frame}

\section{Overview}

\begin{frame}{Overview}
  This presentation is ..
\end{frame}

\section{Random variables}

\begin{frame}{Random variables}
 Discrete random variable is a variable which can have any value from some given
 range with some given probabilty. Example: outcome of throwing a dice. The outcome
 can have any value from 1 to 6, each with the probability 1/6.

 Continuos random variable is a variable which can have any value from some given
 continuous range of values, with some given probabilities for the value been in
 any given interval. Example WRITE THIS!
\end{frame}

\section{Random vectors}

\begin{frame}{Random vectors}
  Random vector is a n-tuple of random variables. Each of these can be either
  similar or different types of variables. More formaly:
  \[ \mathbf{z} =  \begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{pmatrix} \]
  where $\mathbf{z}$ is the random vector and the random variables $z_i$ are
  it's components.
\end{frame}

\section{Probability density}

\begin{frame}{Probability density}
  (Function, Examples)
  \[ p_z(a) = \frac{p(\textnormal{$z$ is in $[a, a + \nu]$})}{\nu}\]
  \[ p_{\mathbf{z}}(\mathbf{a}) =
  \frac{p(\textnormal{$z_i$ is in $[a_i, a_i + \nu]$ for all $i$})}{\nu^n}\]
\end{frame}

\section{Marginal and joint probabilities}

\begin{frame}{Marginal and joint probabilities}
  (Definition, examples)\\
  Marginal:
  \[ p_{z_1}(z_1) =  \int p(z_1, z_2)dz_2\]
  or more rigorosly:
  \[ p_{z_1}(\nu_1) =  \int p(\nu_1, \nu_2)d\nu_2\]
\end{frame}

\section{Conditional probabilities}

\begin{frame}{Conditional probabilities}
  (continuous case, discrete case)
  \[
  p(z_2|z_1 = a) =
  \frac{ p_{\mathbf{z}}(a, z_2)}{\int p_{\mathbf{z}}(a, z_2)dz_2}
  \]
  or more concisely:
  \[
  p(z_2|z_1 = a) =
  \frac{ p_{\mathbf{z}}(a, z_2)}{p_{z_1}(a)} = \text{e.t.c.}
  \]
  for discrete case the integral is replaced by sum, that is
  \[ P_{z_1}(z_1) = \sum_{z_2}P_{\mathbf{z}}(z_1, z_2)\]
\end{frame}

\section{Independence}

\begin{frame}{Independence}
  definition
  \[
  p(z_2|z_1) = p(z_2) \text{for every $z_1$ and $z_2$}
  \]
  which implies that
  \[
  \frac{p(z_1, z_2)}{p(z_1)} = p(z_2) \ \text{or otherway around that} \ p(z_1, z_2) = p(z_1)p(z_2)
  \]
  for all $z_1$ and $z_2$S
\end{frame}

\section{Expectation}

\begin{frame}{Expectation}
  (continuous, discrete (maybe otherway around would be more intuitive?))
  \[
  E\{\mathbf{z}\} = \int p_{\mathbf{z}}(\mathbf{z})\mathbf{z}d\mathbf{z}
  \]
  (write the components?)\\
  its linear:
  \[
  E\{a\mathbf{z} + b\mathbf{z}\}  = aE\{\mathbf{z}\} + bE\{\mathbf{z}\}
  \]
  similarily for any matrix $\mathbf{M}$
  \[
  E\{\mathbf{Mz}\} = \mathbf{M}E\{\mathbf{z}\}
  \]
\end{frame}

\section{Variance, covariance and correlation}

\begin{frame}{Variance, covariance and correlation}
  (dim = 1, covariance matrix)
  \[
  var(z_1)= E\{z_1^2\}-(E\{z_1\})^2 = E\{(z_1-E\{z_1\})^2\}
  \]
  covariance and correlation
  \[
  cov(z_1, z_2) = E\{z_1 z_2\} - E\{z_1\}E\{z_2\}
  \]
  \[
  corr(z_1,z_2) = \frac{cov(z_1, z_2)}{\sqrt{var(z_1)var(z_2)}}
  \]

\end{frame}

\section{Covariance Matrix}

\begin{frame}{Covariance Matrix}
  \[
  \mathbf{C}(\mathbf{z}) =
  \begin{pmatrix}
    cov(z_1, z_1) \ cov(z_1, z_2) \ldots cov(z_1, z_n) \\
    cov(z_2, z_1) \ cov(z_2, z_2) \ldots cov(z_2, z_n) \\
    \vdots \qquad \qquad \ddots \quad \vdots \\
    cov(z_n, z_1) \ cov(z_n, z_2) \ldots cov(z_n, z_n)
  \end{pmatrix}
  \]
  or more concisely:
  \[ \mathbf{C}(\mathbf{z}) =
  E\{\mathbf{zz}^T\} - E\{\mathbf{z}\} E\{\mathbf{z}\}^T\]
\end{frame}

\section{Independence and covariance}

\begin{frame}{Independence and covariance}
  For independent variables $z_1$ and $z_2$ the relation
  \[
  E\{g_1(z_1)g_2(z_2)\} = E\{g_1(z_1)\}E\{g_2(z_2)\}
  \]
  holds for any functions $g_1$ and $g_2$
  (rigorously speaking: for any measurable functions), which implies that
  indepandent variables are uncorreleted (take $g_1(z) = g_2(z) = z$).
\end{frame}

\section{Bayesian inference}

\begin{frame}{Bayesian inference}
  Baye's rule
  \[
  p(\mathbf{s}|\mathbf{z}) =
  \frac{p(\mathbf{z}|\mathbf{s})p_{\mathbf{s}}(\mathbf{s})}{\int p(\mathbf{z}|\mathbf{s})p_{\mathbf{s}}(\mathbf{s})ds}
  \]
  Or more rigorously WRITE HERE?
\end{frame}

\section{Non informative priors}

\begin{frame}{Non informative priors}
  definition
  \[
  p(\mathbf{s}|\mathbf{z}) =
  \frac{p(\mathbf{z}|\mathbf{s})c}{\int p(\mathbf{z}|\mathbf{s})cds} =
  \frac{p(\mathbf{z}|\mathbf{s})}{\int p(\mathbf{z}|\mathbf{s})ds}
  \]
\end{frame}

\section{Bayesian inference as a incremental learning process}

\begin{frame}{Bayesian inference as a incremental learning process}
  Explanation
\end{frame}

\section{Parameter estimation and likelihood}

\begin{frame}{Parameter estimation and likelihood}
  (statistical model, estimation, likelihood \& log-likelihood)
\end{frame}

\section{Maximum likelihood and maximum posteriori}

\begin{frame}{Maximum likelihood and maximum posteriori}
  (definition, definition)
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Template examples}

\begin{frame}{Template examples}
  Examples from the template on the next slides:
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Some of the original default code below: %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Some \LaTeX{} Examples}

\subsection{Tables and Figures}

\begin{frame}{Tables and Figures}

\begin{itemize}
\item Use \texttt{tabular} for basic tables --- see Table~\ref{tab:widgets}, for example.
\item You can upload a figure (JPEG, PNG or PDF) using the files menu.
\item To include it in your document, use the \texttt{includegraphics} command (see the comment below in the source code).
\end{itemize}

% Commands to include a figure:
%\begin{figure}
%\includegraphics[width=\textwidth]{your-figure's-file-name}
%\caption{\label{fig:your-figure}Caption goes here.}
%\end{figure}

\begin{table}
\centering
\begin{tabular}{l|r}
Item & Quantity \\\hline
Widgets & 42 \\
Gadgets & 13
\end{tabular}
\caption{\label{tab:widgets}An example table.}
\end{table}

\end{frame}

\subsection{Mathematics}

\begin{frame}{Readable Mathematics}

Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
$$S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i$$
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.

\end{frame}

\end{document}
