\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}

\begin{document}
\section{Convolutional Networks}
\begin{center}
  \emph{By: Jussi Martin}
\end{center}
\subsection{Preface}
This blogpost is written for the \emph{Image and Video Statistics} study group
held by the AIHelsinki Academia. It is based on the Chapter 9 of the book
\emph{Deep Learning} by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which
is available online at \url{http://www.deeplearningbook.org}.

We will assume the reader to be familiar with basic components of artificial
neural networks, such as weights and biases. Some mathematical preliminaries are
introduced in the text. Our goal is to introduce basics of convolutional neural
networks and explain some of their properties.

Convolutional networks are particularlly usefull in image classification, since
they can learn to be unsensitive to local translations in the input. For example,
convolutional network can learn to classify image of a face correctly dispite
individual faces having slight variation in the positions of facial features.

Besides the classificational aspescts, convolution is also effective in the sence
that it uses fewer connections and shared parameters, which reduces the
computational cost and memory requirements and improves the statistical efficiency.
In deep networks this difference is significant.

\subsection{Definition}
We begin by recalling the mathematical notion of convolution.
Since we are dealing with images, the input data is two dimensional and the convolution
$(I * K)$ is defined as
\[
(I * K)(i, j) = \sum_m \sum_n I(m , n) K(i - m, j - n)
\]
where $I$ is an images, $K$ is a filter applied to it, $i$ and $j$ are the
pixel coordinates. One special property of convolution is that it is commutative, that is
\[
 (K * I) = (K * I).
\]
Which is relatively easy to derive.

In practice the input grid is finite, the filter is defined on a smaller grid and
usually only applied when its coordinates are fully contained in the input grid.

Many machine learning libraries actually defined convolution as
\[
(I * K)(i, j) = \sum_m \sum_n I(i + m , j + n) K(m, n),
\]
which is also known as cross-correlation or convolution without kernel-flipping.
It is not commutative, but from a practical point of view this dose not make any difference
since commutativity is not needed and the network can learn its parameters with
respect to either of these convolutions.

We say that a neural network is convolutional if it uses convolution in place of
general matrix multiplication in at least one of its layers.

\subsection{Convolutional Networks Structure}
Here we give an example layout for convolutional networks structure. We will also
consider what type of outputs convolutional network can have. Individual layers are
presented later.

Typically the first layer in a convolutional network is convolutional. After this
the output is passed trough a nonlinearity, at this point there might be a pooling
layer. This type of structure can then be repeated multiple times. The final layers
are usually fully connected.

Here is an example convolutional network, which was one of the networks used by
the VGG team in the ImageNet 2014 Challenge:

\begin{center}
\begin{tabular}{|c|}
  \hline
  Input 224 $\times$ 224 RGB image\\
  \hline
  convolutional, kernel width 3, channels 64\\
  \hline
  convolutional, kernel width 3, channels 64\\
  \hline
  max pooling\\
  \hline
  convolutional, kernel width 3, channels 128\\
  \hline
  convolutional, kernel width 3, channels 128\\
  \hline
  max pooling\\
  \hline
  convolutional, kernel width 3, channels 256\\
  \hline
  convolutional, kernel width 3, channels 256\\
  \hline
  convolutional, kernel width 3, channels 256\\
  \hline
  convolutional, kernel width 3, channels 256\\
  \hline
  max pooling\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  max pooling\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  convolutional, kernel width 3, channels 512\\
  \hline
  max pooling\\
  \hline
  fully connected, connections 4096\\
  \hline
  fully connected, connections 4096\\
  \hline
  fully connected, connections 1000\\
  \hline
  soft-max\\
  \hline
\end{tabular}
\end{center}
All the hidden layers are followed by ReLu (nonlinearity, defined later in this text),
which is not shown for the sake of brevity.

\subsection{Tensors and Convolutional Layer}
We adopt the Deep Learning book's convention of defining tensors as
multidimensional arrays of real numbers.

Namely, 0-D tensors are just real numbers, 1-D tensors are arrays
\[
T = (T_1, \ldots, T_n)
\]
of real numbers, 2-D tensors are arrays
\[
T = ((T_{1,1}, \ldots, T_{1,n}), \ldots (T_{m,1}, \ldots, T_{m,n}))
\]
of arrays of real numbers (with mutually equal lenght), and so on.

Basically our tensors can be viewed as D-dimensional grids of real numbers.
Example in 2-D:
\[
((1, 2, 3), (4, 5, 6), (7, 8, 9)) \quad \simeq \quad
\begin{array}{|c|c|c|}
  \hline
  1 & 2 & 3 \\
  \hline
  4 & 5 & 6 \\
  \hline
  7 & 8 & 9 \\
  \hline
\end{array}
\]
In general case the sizes of the axes do not need to match.

Typically convolutional network has several channels which may correspond to different
colors in the image or they may originate from same input image which is processed
in different ways in parallel.

We now assume for the sake of simplicity that the input grid has spatial shape of
$a \times a$ grid and the kernel grid has spatial shape of $b \times b$ grid, where
$a$ and $b$ are independent of the channel index and
\[
a = b \cdot N \quad \text{for some $N > 1$}.
\]
We also denote by $c$ and $d$ the number of input and output channels in the layer.

Output of the convolutional layer at pixel $(j, k)$ of the channel $i$ is now
defined as
\[
Z_{i, j, k} = \sum_{l=1}^c \sum_{m = 1}^a \sum_{n=1}^b
V_{l, j + m -1, k + n -1} K_{i, l, m, n}
\]
where $K_{i, l, m, n}$ is the weight of the connection from input in channel $j$
to output in channel $i$, with an offset of $k$ rows and $l$ columns between the
channels.

Also, the output tensor is only defined when
\[
j + m - 1 \le a \quad \text{and} \quad k + n - 1 \le a
\]
for all terms in the summation. This is due to the requierment that the kernel fits
in to the input grid, which now looks different since we are using convolution
without kernel-flipping. Manipulation of these inequalities gives us that
\[
 j \le a - b + 1\quad \text{and} \quad  k \le a - b + 1
\]
Namely, this means that the convolution has shrinked spatial width of the data grid
from $a$ to $a - b + 1$. This is a problem since it opposes a limit to the depth of
the network. It can be however compensated by adding artifiacially zeros to the
boundary of the input layer. This way we can prevent the sizes of the layers shrinking
and obtain deeper networks. The method is known as \emph{zero padding}.

The convolution can also be choosen to accept input from a periodical subgrid.
This is known as convolution with a stride $s$, where we requier that
\[
a = s \cdot M \quad \text{for some $M > 1$}
\]
and defined it as
\[
Z_{i, j, k} = \sum_{l=1}^c \sum_{m = 1}^M \sum_{n=1}^M
V_{l, (j -1)\times s + m, (k-1)\times s + n} K_{i, l, m, n}
\]
with the analogous validicity conditions posed on as before. Similar stride is used
also for pooling which we define later.

\subsection{Nonlinearities}
Some of the typical nonlinearities used in convolutional networks are:

Rectified linear unit also known as \emph{ReLu}, defined by
\[
f(x) = \max(x, 0).
\]

Logistic funtion also known as \emph{sigmoid}, defined by
\[
f(x) = \frac{1}{1 + \exp(-x)}.
\]

Hyperbolic tangent function also known as \emph{tanh}, defined by
\[
f(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}.
\]

A nonlinearity is applied after convolutional layer. Its output is defined as
\[
T_{i,j,k} = f(Z_{i, j, k} + b_{i,j,k})
\]
where $Z_{i, j, k}$ is the output of the convolution and $b_{i, j, k}$ is the bias
unit, which is often shared between units in the same channel, that is
\[
b_{i, j, k} = b_i \quad \text{for all $j$ and $k$}
\]
where $j$ and $k$ are the pixel indicies and $i$ is the channel index.

In some case, for example, if objects in a set of images are known to be in the
center of the images, it may be more convinient to allow the biases to vary with
respect to location.

\subsection{Pooling}
Typically pooling is used for down sampling, which means that the grid size of
the data is reduced. Method that is most often used nowadays is max pooling and
it output is defined as
\[
T_{j,k} = \max \{V_{s \cdot j + \alpha, s \cdot k + \beta} |
\text{ for $(\alpha, \beta) \in [1,\ldots b] \times  [1,\ldots b]$} \}
\]
where $s$ is the stride, $b$ is the width of the max pooling kernel and we have
omited the channel index for the sake of clarity. Often max pooling is used with
$s = b > 1$, which has the efect of shrinking the data grid by factor $s$.

Example in 2-D, max pooling with both $s$ and kernel width set to 2:
\[
\begin{array}{|c|c|c|c|}
  \hline
  0 & 1 & 3 & 9\\
  \hline
  4 & 8 & 6 & 2\\
  \hline
  7 & 4 & 2 & 0\\
  \hline
  4 & 3 & 5 & 1\\
  \hline
 \end{array}
 \quad \stackrel{\text{Pooling}}{\longrightarrow} \quad
 \begin{array}{|c|c|}
   \hline
   8 & 9\\
   \hline
   7 & 5\\
   \hline
  \end{array}
 \]

\subsection{Parse Connections}
One may notice that an output node in a convolutional or a pooling layer is only
conneected to a set of nodes of the size of the convolution or pooling kernel.
Similarly, the input nodes are connected only as many output nodes as is the size
of the kernel (or some times even less if a stride is used). This is in great contrast
to fully connected layers where every node in the input is connected to every node
in the output nodes.


\subsection{Receptive Field}
For a given node the nodes in the previous layers which are directly connected
to it are known as receptive field.

Although the receptive field for a node in the early layers of the network is small
due to sparse connections, it is larger for nodes in the deeper layers of the network
and it can even cover the whole input image if the network has enough layers.

\end{document}
