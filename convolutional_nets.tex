\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}

\begin{document}
\section{Convolutional Networks}
\begin{center}
  \emph{By: Jussi Martin}
\end{center}
\subsection{Preface}
This post is written for the AI Helsinki study group \emph{Image and Video Statistics}.
It is based on the Chapter 9 of the book \emph{Deep Learning} by Ian Goodfellow,
Yoshua Bengio and Aaron Courville, which is under preparation and available
online at \url{http://www.deeplearningbook.org}.

We will assume the reader to be familiar with basic components of artificial
neural networks, such as weights and biases. Some mathematical preliminaries are
introduced in the text. Our goal is to introduce basics of convolutional neural
networks and explain some of their properties.

Convolutional networks are particularlly usefull in image classification, since
they can learn to be unsensitive to local translations in the input. For example,
convolutional network can learn to classify image of a face correctly dispite
individual faces having slight variation in the positions of facial features.

Besides the classificational aspescts, convolution is also effective in the sence
that it uses fewer connections and shared parameters, which reduces the
computational cost and memory requirements. In deep networks this difference is
significant.

% We will examine sparse connectivity and parameter sharing later, after we
% have first introduced tensors and shown how differnt type of convolutions are
% represented with them.

% Skeleton for the post: %

\subsection{Definition}
We begin by introducing the mathematical notion of convolution. Let $x(t)$ and $w(t)$
two functions then we define their convolution $x * w$ as
\[
(x * w)(t) = \int x(a)w(t - a)da.
\]
In practical applications $x$ could be some signal depending on the time $t$ and
$w$ a filter applied to it.

Obiously the data for which the convolution is applied usually is obtained only
as a discrete input, in these cases we define the convolution by using
summation:
\[
(x * w)(t) = \sum_{a = -\infty}^{\infty} x(a)w(t - a).
\]
Moreover, the data might be two dimensional, which is the case with images, then
we use the following definition:
\[
S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(m , n) K(i - m, j - n)
\]
where $I$ and $K$ represent two images and $i$ and $j$ are the pixel coordinates.

In practice the input grid is finite, the filter is defined on a smaller grid and
usually only applied when its coordinates are fully contained in the input grid.
One special property of convolution is that it is commutative, that is
\[
 (K * I) = (K * I).
\]
Which is relatively easy to derive, especially for the discrete cases. Many machine
learning libraries actually defined convolution as
\[
S(i, j) = \sum_m \sum_n I(i + m , j + n) K(m, n),
\]
which is also known as cross-correlation or convolution without kernel-flipping.
It is not commutative, but from practical point of view this dos not make any difference
since commutativity is not needed and the network can learn its parameters with
respect to either of these kernels.

We say that a neural network is convolutional if it has at least one convolutional
layer in it. This layer first applies convolutional filtering to its input data,
after which the data is passed trough a nonlinearity and a pooling stage. Typically
the layer can have several input and output channels which process the data parallely.

\subsection{Tensors and Convolutional Stage}
We adopt the Deep Learning book's convention of defining tensors as
multidimensional arrays of real numbers.

Namely, 0-D tensors are just real numbers, 1-D tensors are arrays
\[
T = (T_1, \ldots, T_n)
\]
of real numbers, 2-D tensors are arrays
\[
T = ((T_{1,1}, \ldots, T_{1,n}), \ldots (T_{m,1}, \ldots, T_{m,n}))
\]
of arrays of real numbers (with mutually equal lenght), and so on.

Basically our tensors can be viewed as D-dimensional grids of real numbers.
Example in 2-D:
\[
((1, 2, 3), (4, 5, 6), (7, 8, 9)) \quad \simeq \quad
\begin{array}{c|c|c}
  1 & 2 & 3 \\
  \hline
  4 & 5 & 6 \\
  \hline
  7 & 8 & 9
 \end{array}
\]
In general case the sizes of the axes do not need to match.

% There are also more elaborate ways of defining tensors, that take into account
% the type of the tensor, but these are not needed for our purposes.

\subsection{(submerge to previous, when ready)}
Output of the convolutional stage at pixel $(j, k)$ of the channel $i$ is

\[
Z_{i, j, k} = \sum_{l, m, n} V_{l, j + m -1, k + n -1} K_{i, l, m, n}
\]
where $K_{i, l, m, n}$ is the weight of the connection from input in channel $j$
to output in channel $i$, both having row index $k$ and column index $l$.\\
With stride:

\[
Z_{i, j, k} = c(K, V, s) =
\sum_{l, m, n} V_{l, (j -1)\times s + m, (k-1)\times s + n} K_{i, l, m, n}
\]
*Tiled (optional?)\\
*Local connections (optional?)\\
*Formulas, etc.\\
... traditional fully connected layer can be seen
as other extreme case, with zero stride and kernel grid size been equal to the
input grid size.

% \subsection{Sparse Connectivity and Parameter Sharing}
% As mentioned earlier, in a convolutional layer there are fewer connections
% between the nodes compared to fully connected layers and the same weights are used
% with multiple connections. We will now illuminate this with some visualizations
% and estimates.
%
% Namely, only as many input nodes as there are weights in the convolution kernel
% are connected to every single node in the output nodes and vice versa.
%
% \[
% \text{Pic: 1-D, connections from inputs to single node in the outputs}
% \]
%
% \[
% \text{Pic: 1-D, connections from single input to nodes in the outputs}
% \]
%
% This is in drastic contrast to fully connected layers where there are connections
% from every input node to a given output node and vice versa.
%
% For example, if we have an image with $28 \times 28$ pixels grid, like the images
% in the MNIST data set have, and we convolve it with $3 \times 3$ filter. Then
% there are only $3 \cdot 3 = 9$ connections from input nodes to single output node,
% where as in fully connected layer there would be $28 \cdot 28 = 784$ connections
% to single output node. And similarly with roles of the input and output nodes reversed.
%
% Here is an illustration of how the weights of the convolution kernel are shared
% between connections:
%
% \[
% \text{Pic: illustration of the parameter sharing}
% \]
%
% Sometimes even the biases are shared in similar manner, but this type of
% parameter sharing is not often used. Also here the difference to fully connected
% layer is significant. If fully connected layer has $n$ input nodes and $m$ output
% nodes, then there are $n \times m$ weight parameters in the layer, where as
% convolutional layer has only as meny weight parameters as the convolution kernel
% has.

\subsection{Nonlinearities}
Some of the typical nonlinearities used in convolutional networks are:

Rectified linear unit also known as \emph{ReLu}, defined by
\[
f(x) = \max(x, 0).
\]

Logistic funtion also known as \emph{sigmoid}, defined by
\[
f(x) = \frac{1}{1 + \exp(-x)}.
\]

Hyperbolic tangent function also known as \emph{tanh}, defined by
\[
f(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}.
\]

A nonlinearity is applied after convolutional stage in a convolutional layer. Its
output is defined as
\[
T_{i,j,k} = f(... + b_{i,j,k})
\]
where $...$ is the output of the convolution and $b_{i, j, k}$ is the bias unit,
which is often shared between units in the same channel, that is
\[
b_{i, j, k} = b_i \quad \text{for all $j$ and $k$}
\]
where $j$ and $k$ are the positional indicies and $i$ is the channel index.

In some case however, for example if objects in a set of images are known to be
in the center of the images, it may be more convinient to allow the biases to vary
with respect to location.

\subsection{(Max) Pooling}
Typically pooling is used for down sampling, which means that the grid size of
the data is reduced.
\[
\begin{array}{c|c|c|c}
  0 & 1 & 3 & 9\\
  \hline
  4 & 8 & 6 & 2\\
  \hline
  7 & 4 & 2 & 0\\
  \hline
  4 & 3 & 5 & 1\\
 \end{array}
 \quad \mapsto \quad
 \begin{array}{c|c}
   8 & 9\\
   \hline
   7 & 5
  \end{array}
 \]
\begin{center}Example in 2-D: max pooling with $2\times2$ kernel and stride = 2.\end{center}
Another benefit of max pooling with down sampling is that it is not so sensitive
to small translations. If we take to grids, which are small translations of each
other, and compare the pooled grid, we see that many of the values are the same.
\[
\text{Pic: Without Shift, Shifted}
\]

\subsection{Zero Padding}
One drawback of using convolutional layers is that they shrink size of the data
grid, which opposes limit to the depth of the network.
\[
\text{Pic: 1-D network with shrinking layers}
\]
This however can be compansated by
adding zeros to the boundary of each neuron layer. That way we can prevent the
sizes of the layers shrinking and obtain deeper networks.
\[
\text{Pic: 1-D network with zero pading and same size layers}
\]
The method discribed above is known as zero padding. One way to do it is to add
just enough zeros that the layer sizes stays the same. Other more extreme option
is to add so many zeros that the convolution can visit every neuron $k$ times,
where $k$ is the size of the convolution kernel.
\[
\text{Pic: 2-D, original layer and layer with minimal padding}
\]
\[
\text{Pic: 2-D, original layer and layer with maximal padding}
\]
One may notice that zero padding makes the outputs near the boundary to have less
information from the inputs, especially if there is maximal amount of padding
used. This may be compensated to some extend if the biases are not shared.

\subsection{Receptive Field}
For a given node the nodes in the previous layers which are directly connected
to it are known as receptive field.

Although the receptive field for single convolutional layer is small due to sparse
connections, it grows when there are more layers added to the network and it can
even cover the whole input grid if the network has enough layers.

\subsection{Convolutional Networks Structure and Output}
Typically the first layers in a convolutional network are convolutional. The final
layers are fully connected and responsible for the one hot encoding of the output.

Optionally the output can be also...

*Output can be a real number (regression, is this without FC layers?)

*Output can be tensor of propabilities of member ship in a class. (This may need
recurent structure in order to work well, check from the book!)

% \subsection{Max Pooling}
% Pictures: Without Shift, Shifted

\subsection{Learned Invariances and Feature Detectors}
Examples, Explanation (no pics)
% \subsection{Pooling with Down Sampling}

% \subsection{Examples of Architectures}
% We give here two examples of convolutional network architectures. Namely, one
% which is fairly classical and another which is from this year (2016).
%
% ...Explanations and pics...

\subsection{Further Topics (Optional ?/ rename as Training?)}
In this last section we will mention briefly some of the topics presented in the
Chapter 9 of the Deep Learning book that we have not yet covered.

One thing we did not yet cover is the training of convolutional network. It turns
out that ... (in order to keep this text concise ... OR: since we assume the reader
to be familiar with basics of regular networks ...) we skip this.

An other topic of possible further interest is the use of recurent sturctures in
a convolutional network, for this we also refer to consult the Deep Learning book.

%\subsection{Convolution with Strides}

%\subsection{Zero Pading Enables Deeper Networks}

% \subsection{Comparison of Local Connections, Convolution, and Full Connections}
% Pictures: Local, Convolution, FC

% \subsection{Partial Connectivity Between Channels}

% \subsection{Tiled Convolution}
% Pictures: Local Connection, Tiled Convolution, Traditional Convolution

% \subsection{Recurent Convolutional Networks}

%\subsection{Gabor Functions (optional)}

%\subsection{Gabor-like Learned Kernels (optional)}

\subsection{Links}
* Our main source:

\url{http://www.deeplearningbook.org}\\
* Video with very basics of convolutional networks:

\url{https://www.youtube.com/watch?v=JiN9p5vWHDY}\\
* Video with slightly more advanced level introduction:

\url{https://www.youtube.com/watch?v=FmpDIaiMIeA}\\
* Video of a conference presentation:

\url{https://www.youtube.com/watch?v=AQirPKrAyDg}



\end{document}
