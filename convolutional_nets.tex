\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}

\begin{document}
\section{Convolutional Networks}
This is a post written for the AI Helsinki study group \emph{Image and Video Statistics}.
It is based on the Chapter 9 of the book \emph{Deep Learning}.

% Skeleton for the post: %

\subsection{Definition}
Convolution in continuous case:
\[
s(t) = \int x(a)w(t - a)da.
\]
Denoted as
\[
s(t) = (x * w)(t).
\]
Convolution in discrete case:
\[
s(t) = (x * w)(t) = \sum_{a = -\infty}^{\infty} x(a)w(t - a).
\]
2-D case:
\[
S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(m , n) K(i - m, j - n).
\]
Commutative:
\[
S(i, j) = (K * I)(i, j) = \sum_m \sum_n I(i - m , j - n) K(m, n).
\]
When defined without kernel-flipping:
\[
S(i, j) = (K * I)(i, j) = \sum_m \sum_n I(i + m , j + n) K(m, n)
\]
(also known as cross-correlation).

Definition: "Convolutional networks are simply neural networks that use
convolution in place of general matrix multiplication in at least one of their
layers"
\subsection{Use of Convolutional Networks}
Used to process data that has a grid-like structure (e.g. images, 1-D: sound?)

\subsection{Benefits}
*Main: Modelling of large inputs while being computationally efficient.\\
*Secondary: Spatial translations of inputs does not add problems.

\subsection{Examples}
(Pic of 2-D convolution, should be earlier?)

\subsection{Sparse Connectivity}

\subsection{Growing Receptive Fields}

\subsection{Parameter Sharing}

\subsection{Convolutional Network Components}

\subsection{Max Pooling}
Pictures: Without Shift, Shifted

\subsection{Example of Learned Invariances}

\subsection{Pooling with Down Sampling}

\subsection{Examples of Architectures}

\subsection{Convolution with Strides}

\subsection{Zero Pading Enables Deeper Networks}

\subsection{Comparison of Local Connections, Convolution, and Full Connections}
Pictures: Local, Convolution, FC

\subsection{Partial Connectivity Between Channels}

\subsection{Tiled Convolution}
Pictures: Local Connection, Tiled Convolution, Traditional Convolution

\subsection{Recurent Convolutional Network}

\subsection{Gabor Functions (optional)}

\subsection{Gabor-like Learned Kernels (optional)}

\section{Tensors}

We adopt the Deep Learning book's convention of defining tensors as
multidimensional arrays of real numbers.

Namely, 0-D tensors are just real numbers, 1-D tensors are arrays
\[
T = (T_1, \ldots, T_n)
\]
of real numbers, 2-D tensors are arrays
\[
T = ((T_{1,1}, \ldots, T_{1,n}), \ldots (T_{m,1}, \ldots, T_{m,n}))
\]
of arrays of real numbers (with mutually equal lenght), and so on.

Basically our tensors can be viewed as D-dimensional grids of real numbers.
Example in 2-D:
\[
((1, 2, 3), (4, 5, 6), (7, 8, 9)) \quad \simeq \quad
\begin{array}{c|c|c}
  1 & 2 & 3 \\
  \hline
  4 & 5 & 6 \\
  \hline
  7 & 8 & 9
 \end{array}
\]
In general the sizes of the axes do not need to be equal.

There are also more elaborate ways of defining tensors, that take into account
the type of the tensor, but these are not needed for our purposes.

\section{References/Links}

\end{document}
