\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}

\begin{document}
\section{Convolutional Networks}
\begin{center}
  \emph{By: Jussi Martin}
\end{center}
\subsection{Preface}
This post is written for the AI Helsinki study group \emph{Image and Video Statistics}.
It is based on the Chapter 9 of the book \emph{Deep Learning} by Ian Goodfellow,
Yoshua Bengio and Aaron Courville, which is under preparation and available
online at \url{http://www.deeplearningbook.org}.

We will assume the reader to be familiar with basic components of artificial
neural networks, such as weights and biases. Some mathematical preliminaries are
introduced in the text.

Our goal is to introduce basic concepts of the convolutional neural networks,
explain where they are used and what kind of benefits the convolutional
structure offers.

% Skeleton for the post: %

\subsection{Definition}
We begin by introducing the mathematical notion of convolution. Let $x(t)$ and $w(t)$
two functions then we define their convolution $x * w$ as
\[
(x * w)(t) = \int x(a)w(t - a)da.
\]
In practical applications $x$ could be some signal depending on the time $t$ and
$w$ a filter applied to it.

Obiously the data for which the convolution is applied usually is obtained only
as a discrete input, in these cases we define the convolution by using
summation:
\[
(x * w)(t) = \sum_{a = -\infty}^{\infty} x(a)w(t - a).
\]
Moreover, the data might be two dimensional, which is the case with images, then
we use the follwing definition:
\[
S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(m , n) K(i - m, j - n)
\]
where $I$ and $K$ represent two images and $i$ and $j$ are the pixel coordinates.

One special property of convolution is that it is commutative, that is
\[
 (K * I) = (K * I).
\]
Which is relatively easy to derive, especially for the discrete cases.
% When defined without kernel-flipping:
% \[
% S(i, j) = \sum_m \sum_n I(i + m , j + n) K(m, n)
% \]
% (also known as cross-correlation).

We say that a neural network is convolutional if it uses convolutional filtering
in at least one of its layers.

\subsection{Use of Convolutional Networks}
Used to process data that has a grid-like structure (e.g. images, 1-D: sound?)

\subsection{Benefits (submerge with previous section?)}
*Modelling of large inputs while being computationally efficient.\\
*Invarians to small translations of inputs.

\subsection{Tensors}
We adopt the Deep Learning book's convention of defining tensors as
multidimensional arrays of real numbers.

Namely, 0-D tensors are just real numbers, 1-D tensors are arrays
\[
T = (T_1, \ldots, T_n)
\]
of real numbers, 2-D tensors are arrays
\[
T = ((T_{1,1}, \ldots, T_{1,n}), \ldots (T_{m,1}, \ldots, T_{m,n}))
\]
of arrays of real numbers (with mutually equal lenght), and so on.

Basically our tensors can be viewed as D-dimensional grids of real numbers.
Example in 2-D:
\[
((1, 2, 3), (4, 5, 6), (7, 8, 9)) \quad \simeq \quad
\begin{array}{c|c|c}
  1 & 2 & 3 \\
  \hline
  4 & 5 & 6 \\
  \hline
  7 & 8 & 9
 \end{array}
\]
In general case the sizes of the axes do not need to match.

% There are also more elaborate ways of defining tensors, that take into account
% the type of the tensor, but these are not needed for our purposes.

\subsection{Convolution via Tensors}
*Stride\\
*Channels\\
*Formulas, etc.

\subsection{Examples (submerge with previous section?)}
(Pic of 2-D convolution, should be earlier?)

\subsection{Sparse Connectivity}
One benefit of using convolutional layers is that there are fewer connections
between the nodes. Only as many input nodes as there are weights in the
convolution kernel are connected to every single node in the output nodes and
vice versa.

\[
\text{Pic: 1-D, connections from inputs to single node in the outputs}
\]

\[
\text{Pic: 1-D, connections from single input to nodes in the outputs}
\]

This is in drastic contrast to fully connected layers where there are connections
from every input node to a given output node and vice versa.

\subsection{Growing Receptive Fields (optional ?)}

\subsection{Parameter Sharing}
Another benefit of using convolution is that there are fewer parameters needed.
Namely the weights of the convolution kernel are shared between similar type of
connection.

\[
\text{Pic: illustration of the parameter sharing}
\]

Sometimes even the biases are shared in similar manner, but this type of
parameter sharing is not often used.

\subsection{Zero Pading}
One drawback of using convolutional layers is that the layers shrink, which
opposes limit to the depth of the network.
\[
\text{Pic: 1-D network with shrinking layers}
\]
This however can be compansated by
adding zeros to the boundary of each neuron layer. That way we can prevent the
sizes of the layers shrinking and obtain deeper networks.
\[
\text{Pic: 1-D network with zero pading and same size layers}
\]
The method discribed above is known as zero padding. One way to do it is to add
just enough zeros that the layer sizes stays the same. Other more extreme option
is to add so many zeros that the convolution can visit every neuron $k$ times,
where $k$ is the size of the convolution kernel.
\[
\text{Pic: 2-D, original layer and layer with minimal pading}
\]
\[
\text{Pic: 2-D, original layer and layer with maximal pading}
\]
One may notice that zero pading makes the outputs near the boundary to have less
information from the inputs, especially if there is maximal amount of pading
used. This may be compensated to some extend if the biases are not shared.
\subsection{(Max) Pooling}
Typically pooling is used for down sampling, which means that the grid size of
the data is reduced.
\[
\begin{array}{c|c|c|c}
  0 & 1 & 3 & 9\\
  \hline
  4 & 8 & 6 & 2\\
  \hline
  7 & 4 & 2 & 0\\
  \hline
  4 & 3 & 5 & 1\\
 \end{array}
 \quad \mapsto \quad
 \begin{array}{c|c}
   8 & 9\\
   \hline
   7 & 5
  \end{array}
 \]
\begin{center}Example in 2-D: max pooling with $2\times2$ kernel and stride = 2.\end{center}
Another benefit of max pooling with down sampling is that it is not so sensitive
to small translations. If we take to grids, which are small translations of each
other, and compare the pooled grid, we see that many of the values are same.
\[
\text{Pic: Without Shift, Shifted}
\]

\subsection{Convolutional Network Components}
Typically: (convolution, Relu, pooling) x $N_1$, followed by FC x $N_2$

% \subsection{Max Pooling}
% Pictures: Without Shift, Shifted

\subsection{Example of Learned Invariances}

% \subsection{Pooling with Down Sampling}

\subsection{Examples of Architectures}
Explanations and pics

\subsection{Further Topics}
In this last section we will mention briefly some of the topics presented in the
Chapter 9 of the Deep Learning book that we have not yet covered.

There are variants of the usual way of doing the convolution...
\newline
*Comparison of Local Connections, Convolution, and FC (Optional)\\
*Tiled Convolution (optional)\\
*Recurent Convolutional Networks

%\subsection{Convolution with Strides}

%\subsection{Zero Pading Enables Deeper Networks}

% \subsection{Comparison of Local Connections, Convolution, and Full Connections}
% Pictures: Local, Convolution, FC

% \subsection{Partial Connectivity Between Channels}

% \subsection{Tiled Convolution}
% Pictures: Local Connection, Tiled Convolution, Traditional Convolution

% \subsection{Recurent Convolutional Networks}

%\subsection{Gabor Functions (optional)}

%\subsection{Gabor-like Learned Kernels (optional)}

\subsection{Links}
* Our main source:

\url{http://www.deeplearningbook.org}\\
* Video with very basics of convolutional networks:

\url{https://www.youtube.com/watch?v=JiN9p5vWHDY}\\
* Video with slightly more advanced level introduction:

\url{https://www.youtube.com/watch?v=FmpDIaiMIeA}\\
* Video of a conference presentation:

\url{https://www.youtube.com/watch?v=AQirPKrAyDg}



\end{document}
